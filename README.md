# ETL Pipeline: Real-time News Article Processing

This project implements an **ETL (Extract, Transform, Load)** pipeline for processing real-time cryptocurrency-related news articles. Built for the **Big Data Integration and Storage (PROG8451)** course, this system demonstrates streaming data processing using **Kafka**, **Spark Structured Streaming**, and **MySQL**.

## ðŸ“Œ Overview

The pipeline:

- **Extracts** JSON-formatted news articles via an API
- **Transforms** data using Spark (schema enforcement, timestamp conversion, etc.)
- **Loads** clean records into a MySQL database for querying

---

## ðŸ”§ Technologies Used

- **Kafka** â€“ Distributed messaging system for real-time data
- **Spark Structured Streaming** â€“ Data transformation and stream processing
- **Python** â€“ API interaction and Kafka producer
- **MySQL** â€“ Final data storage for analytics
- **Scala** â€“ Used for Spark transformations

---

## ðŸ§© Pipeline Architecture

```mermaid
graph LR
    A[Python Producer] --> B[Kafka (finaltopic)]
    B --> C[Spark Structured Streaming]
    C --> D[MySQL (final.articles)]
