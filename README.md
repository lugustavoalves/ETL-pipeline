# ETL Pipeline: Real-time News Article Processing

This project implements an **ETL (Extract, Transform, Load)** pipeline for processing real-time cryptocurrency-related news articles.This system demonstrates streaming data processing using **Kafka**, **Spark Structured Streaming**, and **MySQL**.

---

## ğŸ“Œ Overview

The pipeline:

- **Extracts** JSON-formatted news articles via an API
- **Transforms** data using Spark (schema enforcement, timestamp conversion, etc.)
- **Loads** clean records into a MySQL database for querying

---

## ğŸ”§ Technologies Used

- **Kafka** â€“ Distributed messaging system for real-time data
- **Spark Structured Streaming** â€“ Data transformation and stream processing
- **Python** â€“ API interaction and Kafka producer
- **MySQL** â€“ Final data storage for analytics
- **Scala** â€“ Used for Spark transformations

---

## ğŸ§© Pipeline Architecture

![Pipeline](images/1.png)

| Component              | Role                                                                 |
|------------------------|----------------------------------------------------------------------|
| `source_data.py`       | Extracts articles from NewsAPI and saves to `articles.json`          |
| `producer.py`          | Monitors `articles.json`, transforms data, and streams to Kafka      |
| `Kafka (finaltopic)`   | Message queue for real-time article delivery                         |
| `Spark (Scala)`        | Transforms messages and writes structured data to MySQL              |
| `MySQL`                | Stores cleaned and structured news articles                          |

---

## ğŸš€ Pipeline Stages

### 1. Data Extraction (Python)
- **Script**: `source_data.py`
- Connects to NewsAPI and fetches news articles with the keywords:
  `bitcoin`, `cryptocurrency`, `crypto`, `BTC`, `btc`
- Saves raw data to a local file: `articles.json`

 ![articles.json Output](images/2.png)

---

### 2. Kafka Producer
- **Script**: `producer.py`
- Monitors the `articles.json` file for changes
- Performs the following:
  - Removes `urlToImage`
  - Converts `publishedAt` to datetime
  - Converts the `source` field into a key-value format
- Streams the cleaned JSON to the Kafka topic `finaltopic`

 ![Producer Output](images/3.png)

---

### 3. Data Transformation (Spark Structured Streaming)
- **Language**: Scala
- Spark reads from the Kafka topic, applies schema and transformations:
  - Enforces types like `StringType` and `TimestampType`
  - Handles nullable fields like `author` and `description`

 ![Spark Console Output](images/4.png)

---

### 4. Data Loading (MySQL)
- **Sink**: MySQL database
- Final structured records are written to `final.articles` table
- Example SQL Query to validate:

```sql
SELECT * FROM articles LIMIT 5;
```

 ![MySQL Data Output](images/5.png)


---

## ğŸ“ Folder Structure

```
/project-folder
â”‚
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ 1.png
â”‚   â”œâ”€â”€ 2.png
â”‚   â”œâ”€â”€ 3.png
â”‚   â”œâ”€â”€ 4.png
â”‚   â””â”€â”€ 5.png
â”‚
â”œâ”€â”€ json_consumer_hdfs.py
â”œâ”€â”€ json_consumer.py
â”œâ”€â”€ producer.py
â”œâ”€â”€ source_data.py
â”œâ”€â”€ sparkscala
â””â”€â”€ README.md
```
 
---
